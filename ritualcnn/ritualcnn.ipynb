{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe651b17-7151-45b7-a282-0140fecd61dc",
   "metadata": {},
   "source": [
    "## 0.1 - Every imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bac23e-846e-4c9d-a204-829aaa66fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tf2onnx\n",
    "import onnx\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "import torch.onnx\n",
    "import random\n",
    "from PIL import ImageOps, ImageEnhance\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageOps, ImageEnhance, ImageChops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39058f4f-47b9-445e-9db3-58065dc06419",
   "metadata": {},
   "source": [
    "## 0.2 - Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f06189d-dfef-4e5f-88ed-e86f742bec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "input_shape = 128\n",
    "directory_path = './data'\n",
    "augmented_path = f\"{directory_path}_augment\"\n",
    "test_dir = \"test\"\n",
    "onnx_model_path = \"rituals.onnx\"\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "num_augmented_per_image = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d352f38-dee9-45f7-ad65-32db8ccb15bb",
   "metadata": {},
   "source": [
    "## 0.3 - Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffe97ec-1ee2-417b-b3bc-2e96cecb0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filename):\n",
    "    img = Image.open(filename).convert('L')  # Conversion en niveaux de gris\n",
    "    img = img.resize((input_shape, input_shape))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    # Seuillage binaire\n",
    "    img = np.where(img > 127, 255.0, 0.0)\n",
    "    return img\n",
    "\n",
    "def load_images_from_directory(directory_path, max_images_per_label=None):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    label_names = []\n",
    "    \n",
    "    for label in os.listdir(directory_path):\n",
    "        label_path = os.path.join(directory_path, label)\n",
    "        # Ignorer les fichiers cachés comme .DS_Store\n",
    "        if label.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        print(f\"Actually load : {label} with size : {len(os.listdir(label_path))}\")\n",
    "        if os.path.isdir(label_path):\n",
    "            label_image_count = 0\n",
    "            \n",
    "            for filename in os.listdir(label_path):\n",
    "                # Ignorer les fichiers cachés comme .DS_Store\n",
    "                if filename.startswith('.'):\n",
    "                    continue\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    if max_images_per_label is not None and label_image_count >= max_images_per_label:\n",
    "                        break\n",
    "                    image_paths.append(os.path.join(label_path, filename))\n",
    "                    labels.append(label)\n",
    "                    label_image_count += 1\n",
    "                    if label not in label_names:\n",
    "                        label_names.append(label)\n",
    "    \n",
    "    label_names.sort()\n",
    "    return image_paths, labels, label_names\n",
    "\n",
    "\n",
    "def images_to_numpy(image_paths, labels, label_names):\n",
    "    images = []\n",
    "    for path in tqdm(image_paths, desc=\"Processing images\", unit=\"image\"):\n",
    "        image = load_and_preprocess_image(path)\n",
    "        images.append(image)\n",
    "    \n",
    "    labels = [label_names.index(label) for label in labels]\n",
    "    \n",
    "    print(\"Converting images to NumPy array...\")\n",
    "    images = np.array(images)\n",
    "    return images, np.array(labels)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = torch.FloatTensor(images).unsqueeze(1) / 255.0  # Normalisation [0, 1]\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "class RitualCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RitualCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "# Fonction de validation\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "def augment_image(img):\n",
    "    # Rotation aléatoire de -15 à +15 degrés\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        img = img.rotate(angle)\n",
    "    \n",
    "    # Translation légère\n",
    "    if random.random() < 0.5:\n",
    "        max_shift = int(0.1 * input_shape)\n",
    "        x_shift = random.randint(-max_shift, max_shift)\n",
    "        y_shift = random.randint(-max_shift, max_shift)\n",
    "        img = ImageChops.offset(img, x_shift, y_shift)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ade71-9d59-4b4e-bcbf-819ed6941f94",
   "metadata": {},
   "source": [
    "## 0.4 - Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499d48c6-7d90-462a-bf0a-4059f9cc4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete. Images saved in ./data_augment\n"
     ]
    }
   ],
   "source": [
    "for label in os.listdir(directory_path):\n",
    "    if label.startswith('.'):\n",
    "        continue\n",
    "    label_dir = os.path.join(directory_path, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        aug_label_dir = os.path.join(augmented_path, label)\n",
    "        os.makedirs(aug_label_dir, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(label_dir):\n",
    "            if filename.startswith('.') or not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            filepath = os.path.join(label_dir, filename)\n",
    "            try:\n",
    "                img = Image.open(filepath).convert('L')\n",
    "                img = img.resize((input_shape, input_shape))\n",
    "                \n",
    "                # Save original image to augmented folder\n",
    "                base_name, ext = os.path.splitext(filename)\n",
    "                img.save(os.path.join(aug_label_dir, f\"{base_name}_orig{ext}\"))\n",
    "                \n",
    "                # Generate augmented images\n",
    "                for i in range(num_augmented_per_image):\n",
    "                    aug_img = augment_image(img)\n",
    "                    aug_img.save(os.path.join(aug_label_dir, f\"{base_name}_aug{i+1}{ext}\"))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "print(f\"Augmentation complete. Images saved in {augmented_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb22f48-f722-42c1-b6ff-44d3207c6669",
   "metadata": {},
   "source": [
    "## 1.0 - Collect images and classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08751c-c223-4783-9182-70abced08bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images_per_label = 1000000\n",
    "\n",
    "image_paths, labels, label_names = load_images_from_directory(\n",
    "    augmented_path, \n",
    "    max_images_per_label=max_images_per_label\n",
    ")\n",
    "images, labels = images_to_numpy(image_paths, labels, label_names)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageDataset(X_train, y_train)\n",
    "test_dataset = ImageDataset(X_test, y_test)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "num_classes = len(label_names)\n",
    "print(f\"Total images: {len(images)}\")\n",
    "print(f\"Training images: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {label_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23247e1-9645-4503-b333-288577aa392b",
   "metadata": {},
   "source": [
    "## 2.0 - Define model & function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1db48-768d-4acb-a832-35ea39cfcc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RitualCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7\n",
    ")\n",
    "summary(model, input_size=(1, input_shape, input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d28e96-aff6-4957-8a69-7d433bbbebd0",
   "metadata": {},
   "source": [
    "## 2.1 - Training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532a524-c9d5-4bd5-a579-16e8b6934049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    if new_lr != old_lr:\n",
    "        print(f\"Learning rate reduced: {old_lr:.2e} -> {new_lr:.2e}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rituals_pytorch.pth')\n",
    "        print(\"Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e10f75-ae95-4af6-b716-b9df8506cb80",
   "metadata": {},
   "source": [
    "## 2.2 - Save to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9e9a4-af8b-4d93-b146-46722134f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('rituals_pytorch.pth'))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, input_shape, input_shape).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                             \n",
    "    dummy_input,                       \n",
    "    'rituals_model.onnx',            \n",
    "    export_params=True,             \n",
    "    opset_version=11,                  \n",
    "    do_constant_folding=True,        \n",
    "    input_names=['input'],           \n",
    "    output_names=['output'],           \n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},    \n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Modèle exporté en ONNX : rituals_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5bf12-5072-4ca2-8081-6563fc206f28",
   "metadata": {},
   "source": [
    "## 2.3 - Test with your new datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7c4aa-5b4f-47b1-b6e7-02ba5ccfc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model.load_state_dict(torch.load('rituals_pytorch.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def predict_image(image_path, model, device):\n",
    "    \"\"\"Prédit la classe d'une image\"\"\"\n",
    "    img = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    img_tensor = torch.FloatTensor(img).unsqueeze(0).unsqueeze(0) / 255.0\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    return predicted.item(), confidence.item(), probabilities[0].cpu().numpy()\n",
    "\n",
    "def test_on_folder(test_folder_path, model, label_names, device, num_images=16):\n",
    "    \"\"\"Teste le modèle sur un dossier et affiche les résultats\"\"\"\n",
    "    \n",
    "    all_images = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for label in os.listdir(test_folder_path):\n",
    "        label_path = os.path.join(test_folder_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in os.listdir(label_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    all_images.append(os.path.join(label_path, filename))\n",
    "                    true_labels.append(label)\n",
    "    \n",
    "    indices = np.random.choice(len(all_images), min(num_images, len(all_images)), replace=False)\n",
    "    \n",
    "    rows = int(np.sqrt(num_images))\n",
    "    cols = int(np.ceil(num_images / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for idx, img_idx in enumerate(indices):\n",
    "        image_path = all_images[img_idx]\n",
    "        true_label = true_labels[img_idx]\n",
    "        \n",
    "        pred_idx, confidence, probabilities = predict_image(image_path, model, device)\n",
    "        pred_label = label_names[pred_idx]\n",
    "        \n",
    "        img_display = Image.open(image_path).convert('L')\n",
    "        \n",
    "        is_correct = (pred_label == true_label)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        axes[idx].imshow(img_display, cmap='gray')\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        color = 'green' if is_correct else 'red'\n",
    "        title = f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2%}\"\n",
    "        axes[idx].set_title(title, fontsize=10, color=color, weight='bold')\n",
    "    \n",
    "    for idx in range(len(indices), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"\\nAccuracy on sample: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "test_folder_path = \"test\" \n",
    "accuracy = test_on_folder(test_folder_path, model, label_names, device, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd7043-2f3b-42ec-815e-4993f1e2b01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dc682-7cce-43cc-9434-9ff8e066b207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
